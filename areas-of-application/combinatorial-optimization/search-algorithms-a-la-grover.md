# Search algorithms à la Grover

## Overview

Grover's search algorithm [@grover1996QSearch], and its generalizations, such as [amplitude amplification](../../quantum-algorithmic-primitives/amplitude-amplification-and-estimation/amplitude-amplification.md#amplitude-amplification), are essential sources of quantum speedups. A straightforward application of Grover search in the spirit of optimization is quantum minimum finding [@durr1996QMinimumFinding] that finds the minimizer of a function on a given set of elements with a quadratic speedup, and its natural generalization analogous to [amplitude amplification](../../quantum-algorithmic-primitives/amplitude-amplification-and-estimation/amplitude-amplification.md#amplitude-amplification) can be found in [@apeldoorn2017QSDPSolvers].


As search is a very generic primitive, Grover's algorithm is extremely widely applicable and it can speed up many subroutines especially in algorithms for combinatorial optimization. In the early days of quantum computing, a plethora of such applications were found, and the list still keeps growing. We list a few such representative applications that demonstrate how Grover's algorithm may be applied to speed up combinatorial optimization.


## Actual end-to-end problem(s) solved

The goal is to solve a search problem, i.e., decide whether there is an element among a set of objects that satisfies some criterion, and if there is such an object, find one. Many combinatorial optimization problems are fundamentally search problems; a notable class of examples are graph problems, such as finding a maximal independent set, a $k$-coloring, a lowest weight Hamiltonian cycle (called the traveling salesperson problem), or the shortest path between two vertices.


For conceptual clarity, here, we focus on the prototypical Boolean satisfiability problem, i.e., $\mathsf{SAT}$ solving: given a Boolean formula in the so-called *conjunctive normal form*, decide whether it has a satisfying Boolean assignment (and if so, find one). A formula in this form consists of some constraints (called *clauses*) each containing the logical AND of some variables or their negation (called *literals*). We denote the number of Boolean variables by $n$, while the total number of literals of the formula by $\ell$ (counted with multiplicity).


## Dominant resource cost/complexity

If there are at least $m$ marked elements among $N$ possible ones, then the search problem can be solved with high probability by using $\mathcal{O}\left( \sqrt{N/m} \right)$ Grover iterations. Each Grover iteration requires generating a uniform superposition over the $N$ elements starting from the all $0$ state, and to check whether an element is marked (in superposition), which can be implemented with gate cost $\mathcal{O}\left( \ell+n \right)$. If the formula is satisfiable then there is at least one solution, thus $\mathcal{O}\left( \sqrt{2^n} \right)$ Grover iterations suffice, giving an overall complexity of $\mathcal{O}\left( (\ell+n)\sqrt{2^n} \right)$.


In some applications, it is useful to consider a generalization of Grover search, [amplitude amplification](../../quantum-algorithmic-primitives/amplitude-amplification-and-estimation/amplitude-amplification.md#amplitude-amplification), which enables working with an arbitrary prior distribution on the elements, unlike Grover's algorithm which effectively uses a uniform prior. The relevance of this extension can be seen through the example of 3-$\mathsf{SAT}$, which is a restricted version of $\mathsf{SAT}$ where each clause has at most 3 literals. A clever application of [amplitude amplification](../../quantum-algorithmic-primitives/amplitude-amplification-and-estimation/amplitude-amplification.md#amplitude-amplification) described by Ambainis [@ambainis2004QSearchAlgos] for solving 3-$\mathsf{SAT}$ more efficiently uses Schöning's algorithm [@schoning1999ProbAlgForkSAT] and thus generates a nontrivial prior distribution on the solutions.


The complexity of [amplitude amplification](../../quantum-algorithmic-primitives/amplitude-amplification-and-estimation/amplitude-amplification.md#amplitude-amplification) is similar to that of Grover's search in general. If $\ket{\psi}$ is the quantum state representing the prior distribution, so that measuring the state yields a marked element with probability at least $p$, then $\mathcal{O}\left( \sqrt{1/p} \right)$ "Grover iterations" suffice to find a marked element with high probability. The algorithm requires preparing the initial state $\ket{\psi}$ and then each iteration consist of a reflection $2\ketbra{\psi}{\psi}-I$ around $\ket{\psi}$ and checking whether an element is marked (in superposition). The former reflection can be implemented with two uses of the circuit that prepares $\ket{\psi}$ from the all $0$ state, and a reflection about the all $0$ state.


## Existing error corrected resource estimates

There are several studies on the resource estimation of Grover-type (sub)quadratic speedups. Due to the wide range of these problems, we do not focus on explicit gate counts on any particular problem/implementation variant, but rather list some prominent articles and illustrate their findings on a high level [@campbell2019ApplyingQToCSPs; @sanders2020FTQCforCombOpt; @babbush2021FocusBeyondQuadratic; @cade2022quantifying; @cade2022quantum; @hoefler2023RealAchievQAdvantage]. Unfortunately, these recent studies revealed that quadratic or smaller speedups alone are unlikely to be useful probably even in the medium term, unless the large overheads of [fault-tolerant quantum computing](../../fault-tolerant-quantum-computation/introduction.md#fault-tolerant-quantum-computation) schemes can be greatly reduced. For example, [@sanders2020FTQCforCombOpt] concluded that even if there is some reasonable advantage in quantum gate counts for solving the constraint satisfaction problems that they consider, the classical computation supporting the [fault-tolerant quantum computation](../../fault-tolerant-quantum-computation/introduction.md#fault-tolerant-quantum-computation) actually annihilates the speedup in practice. They state that "Even when considering only problem instances that can be solved within one day, we find that there are potentially large quantum speedups available. $\ldots$ However, the number of physical qubits used is extremely large, $\ldots$ In particular, the quantum advantage disappears if one includes the cost of the classical processing power required to perform decoding of the surface code using current techniques." The most recent of the above quoted papers [@hoefler2023RealAchievQAdvantage] estimates that getting a quantum advantage via a quadratic speedup requires at least a month-long computation already if each iteration contains at least one floating-point operation. The situation looks more promising for cubic and quartic speedups, but unfortunately such improvements seem to require [techniques beyond Grover search](../../areas-of-application/combinatorial-optimization/beyond-quadratic-speedups-in-exact-combinatorial-optimization.md#beyond-quadratic-speedups-in-exact-combinatorial-optimization).


## Caveats

Grover originally described his result as "A fast quantum mechanical algorithm for database search" [@grover1996QSearch]. If we work in the circuit model of quantum computation, then strictly speaking Grover search gives a slowdown for database search, as every Grover iteration needs to "touch" every element in the database. If we anyway need to touch all $N$ elements in the database, then the best we can do is to simply go over every element in linear time $\mathcal{O}\left( N \right)$. Grover's search circuit in this case would have gate complexity $\widetilde{\mathcal{O}}\left( N^{3/2} \right)$, clearly worse than sequentially going through the entire dataset.


In the database scenario, we can only recover the quadratic speedup if we assume that we can use a [quantum random access memory](../../quantum-algorithmic-primitives/loading-classical-data/quantum-random-access-memory.md#quantum-random-access-memory) (QRAM), with constant (or logarithmic) cost for each database query. The analogous assumption regarding ordinary RAM is often made in classical computer science, simply because RAM calls are cheap in practice. However, since a RAM call should be able to touch every bit of the database, from a circuit complexity perspective a RAM call must have gate cost at least $N$. On the other hand, this task can be parallelized very well, so with appropriate hardware it is reasonable to count a RAM call to have (time) complexity $\log(N)$. While [QRAM](../../quantum-algorithmic-primitives/loading-classical-data/quantum-random-access-memory.md#quantum-random-access-memory) can also be implemented with a quantum circuit of $\mathcal{O}\left( \log(N) \right)$ depth, a similar accounting might not be fair in the case of [QRAM](../../quantum-algorithmic-primitives/loading-classical-data/quantum-random-access-memory.md#quantum-random-access-memory) if error correction is necessary, especially if one implements the entire QRAM circuit in a [fault-tolerant](../../fault-tolerant-quantum-computation/introduction.md#fault-tolerant-quantum-computation) fashion.


However, Grover's algorithm can provide a quadratic speedup without extra hardware assumptions when the elements of the list that we search over can be easily generated and checked "on the fly." For example, in the case of $\mathsf{SAT}$, we search over the $2^n$ possible truth assignments, yet we can easily check whether an individual assignment is satisfactory by simply substituting the assignment into the formula and evaluating the resulting Boolean expression.


## Comparable classical complexity

For the unstructured search problem, exhaustive search is essentially the best that can be done, with a running time $\sim\ell \cdot 2^n$. Of course, $\mathsf{SAT}$ seems to be far from unstructured, but under the Strong Exponential-Time Hypothesis [@impagliazzo2001WhichProbHasSETC; @calabro2009SETH] the best classical algorithm for $\mathsf{SAT}$ has running time $2^{n-o(n)}$.


A similar argument holds for the generalized problem considered in the setting of [amplitude amplification](../../quantum-algorithmic-primitives/amplitude-amplification-and-estimation/amplitude-amplification.md#amplitude-amplification): if we have some prior distribution, we can classically find a marked element by sampling from this distribution about $\sim \frac{1}{p}$ times. Since unstructured search is a special case of this problem, we cannot hope for a better classical algorithm in general.


## Speedup

The speedup is quadratic in terms of the number of required iterations if we compare to corresponding naive classical algorithms. It can be shown that this speedup is optimal in the black-box query model [@bennett1997QSearchLowerBound]. Moreover, we do not expect that there would be a bigger than quadratic speedup in gate complexity [@buhrman2021QSETH] in the general (non-black-box) case.


## Outlook

We have discussed how Grover search provides a quadratic speedup for $\mathsf{SAT}$, and how [amplitude amplification](../../quantum-algorithmic-primitives/amplitude-amplification-and-estimation/amplitude-amplification.md#amplitude-amplification) yields a quadratic speedup for 3-$\mathsf{SAT}$. Grover's algorithm can be used as a subroutine in several other combinatorial optimization problems as well, e.g., related to graphs. In the literature, these problems are most often studied in the query model, therefore here we also only discuss their speedup in terms of query complexity. Since these are (sub)quadratic speedups, we know that the fault-tolerant resource estimates will be unfavorable anyway, as discussed above.


For example, the problem of finding the shortest paths from a single source $s$ in graph $G=(V,E)$ to all other vertices $v\in V$ can be solved using Dijkstra's algorithm in time $\mathcal{O}\left( |E|+|V|\log|V| \right)$ if the graph is provided with its adjacency list (and with query complexity $\mathcal{O}\left( |E| \right)$), whereas the quantum query complexity of this problem is $\widetilde{\Theta}(\sqrt{|V||E|})$ [@durr2004QQueryCompGraph]. The paper [@durr2004QQueryCompGraph] determines the query complexity of several other graph problems such as deciding graph connectivity and strong connectivity as well as finding the minimum-weight spanning tree. For all of these problems, there is a similar moderate (sub)quadratic quantum speedup.


One graph problem that is often mentioned in connection to quantum computation is the (in)famous traveling salesperson problem. However, for this problem, the best provable speedup is only subquadratic. The naive classical problem runs in time $n!$, and Grover's algorithm offers a quadratic speedup over it. Unfortunately, the best classical algorithm uses dynamic programming and runs in time $2^n$. Ambainis et al. [@ambainis2019QSpeedUpExpTimeDPAlgs] showed how to obtain a speedup over this algorithm by combining classical precalculation with recursive applications of Grover's search resulting in time complexity $\widetilde{\mathcal{O}}\left( 1.817^n \right)$ assuming that [QRAM](../../quantum-algorithmic-primitives/loading-classical-data/quantum-random-access-memory.md#quantum-random-access-memory) calls have unit costs. Considering the overheads coming from the implementation of [QRAM](../../quantum-algorithmic-primitives/loading-classical-data/quantum-random-access-memory.md#quantum-random-access-memory) and [fault tolerance](../../fault-tolerant-quantum-computation/introduction.md#fault-tolerant-quantum-computation), the traveling salesperson problem seems to be one of the *least* likely candidates to achieve a practical quantum speedup.


Finally, let us mention quantum walk algorithms, which can also be viewed as a generalization of Grover's search. However, quantum walks are more distant relatives of Grover's search and can only be applied in more specific settings. They can be used for proving many nontrivial speedups in query complexity, however the resulting algorithms are often not practical due to high space and/or gate complexity overheads, as is the case for the prototypical Element Distinctness problem. The query reduction is moderate $N^2\rightarrow N^{4/3}$ in the number of elements $N$, but the corresponding quantum algorithm [@ambainis2004QWalkForElementDist] unfortunately uses a [QRAM](../../quantum-algorithmic-primitives/loading-classical-data/quantum-random-access-memory.md#quantum-random-access-memory) consisting of about $\sim N^{4/3}$ registers.


There are nevertheless more practical quantum walk algorithms applicable, e.g., to speed up backtracking algorithms [@montanaro2015QuantumBacktracking; @ambainis2017TreeSizeEstimation; @jarret2017ImprovedQBacktracking; @martiel2020practicalBacktracking], which are among the most successful and widely used classical heuristics for solving $\mathsf{SAT}$ instances in practice. The quantum algorithm can achieve an essentially quadratic speedup compared to its classical backtracking variant. This approach is applicable to the traveling salesperson problem in the special case that the graph has degree at most 4 [@moylett2017travelingSalemsanBoundedDegree]. For resource estimates see the earlier quoted reference [@campbell2019ApplyingQToCSPs]. A further extension of this algorithm is applicable to branch-and-bound algorithms [@montanaro2019QBranchAndBound; @chakrabarti2022universal], and in some cases yields running times that are substantially better than what we know can be achieved by naively using Grover's algorithm. For example, it can find exact ground states for most instances of the Sherrington–Kirkpatrick model [@sherrington1975solvable] in time $\mathcal{O}\left( 2^{0.226n} \right)$ [@montanaro2019QBranchAndBound], which means about a quadratic speedup compared to classical methods. Branch-and-bound-based speedups can also be applied to solve mixed-integer programs, which includes certain formulations of the [portfolio optimization](../../areas-of-application/finance/portfolio-optimization.md#portfolio-optimization) problem [@chakrabarti2022universal].


There is a plethora of other applications of quantum search speedups, ranging from machine learning [@wiebe2015QNearestNeighbour] to dynamical programming solutions of other NP-hard problems [@ambainis2019QSpeedUpExpTimeDPAlgs], which we do not discuss here for length constraints and due to discouraging resource estimates for (sub)quadratic quantum speedups. 





