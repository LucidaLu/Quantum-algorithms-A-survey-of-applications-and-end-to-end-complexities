# Conic programming: Solving LPs, SOCPs, and SDPs

## Overview

Conic programs are a specific subclass of convex optimization problems, where the objective function is linear and the convex constraints are restrictions to the intersection of affine spaces and certain cones within $\mathbb{R}^n$. Commonly considered cones are the positive orthant, the second-order cone ("ice-cream cone"), and the semidefinite cone, which give rise to linear programs (LPs), second-order cone programs (SOCPs), and semidefinite programs (SDPs), respectively. This framework remains quite general and many real-world problems can be reduced to a conic program. However, the additional structure of the program allows for more efficient classical and quantum algorithms, compared to [completely general convex problems](../../areas-of-application/continuous-optimization/general-convex-optimization.md#general-convex-optimization).


Algorithms for LPs, SOCPs, and SDPs have long been a topic of study. Today, the best classical algorithms are based on [interior point methods](../../quantum-algorithmic-primitives/quantum-interior-point-methods.md#quantum-interior-point-methods) (IPMs), but other algorithms based on the [multiplicative weights update](../../quantum-algorithmic-primitives/multiplicative-weights-update-method.md#multiplicative-weights-update-method) (MWU) method exist and can be superior in a regime where high precision is not required. Both of these approaches can be turned into quantum algorithms with potential to deliver asymptotic quantum speedup for general LPs, SOCPs, and SDPs. However, the runtime of the quantum algorithm typically depends on additional instance-specific parameters, which makes it difficult to produce a general apples-to-apples comparison with classical algorithms.


## Actual end-to-end problem(s) solved

- Linear programs (LPs) are the simplest convex program. An LP instance is specified by an $m \times n$ matrix $A$, an $n$-dimensional vector $c$ and an $m$-dimensional vector $b$. The problem can then be written as $$\begin{equation} \label{eq:LP} \begin{align} & \min_{x \in \mathbb{R}^n} \langle c, x \rangle \\
  \text{subject to } & Ax=b \\
  & x_i \geq 0 \text{ for } i=1,\ldots,n \end{align} \end{equation}$$ where notation $\langle u, v \rangle$ denotes the standard dot product of vectors $u$ and $v$. The function $\langle c, x\rangle$, which is linear in $x$, is called the objective function, and a point $x$ is called feasible if it satisfies the linear equality[^1] constraints $Ax=b$ as well as the positivity constraints $x_i \geq 0$ for all $i$. We denote the feasible point that optimizes the objective function by $x^*$. Let $\epsilon$ be a precision parameter. The actual end-to-end problem solved is to take as input a classical description of the problem instance $(c,A,b,\epsilon)$ and output a classical description of a point $x$ for which $\langle c, x \rangle \leq \langle c, x^*\rangle + \epsilon$. The set of points that obey the positivity constraints $x_i \geq 0$ forms the positive orthant of the vector space $\mathbb{R}^n$. This set meets the mathematical definition of a convex cone: for any points $u$ and $v$ in the set and any non-negative scalars $\alpha,\beta\geq 0$, the point $\alpha u+\beta v$ is also in the set.
- Second-order cone programs (SOCPs) are formed by replacing the positivity constraints in the definition of LPs with one or more second-order cone constraints, where the second-order cone of dimension $k$ is defined to include points $(x_0;x_1;\ldots;x_{k-1}) \in \mathbb{R}^k$ for which $x_0^2 \geq x_1^2+\ldots + x_{k-1}^2$.
- Semidefinite programs (SDPs) are formed by replacing the $n$-dimensional vector $x$ in the definition of LPs with a $n \times n$ symmetric matrix $X$ and replacing the positive orthant constraint with the conic constraint that $X$ is a positive semidefinite matrix. Denote the set of $n \times n$ symmetric matrices by $\mathbb{S}^n$, and for any pair of matrices $U,V \in \mathbb{S}^n$, define the notation $\langle U, V\rangle = \text{tr}(UV)$ (which generalizes the standard dot product). Then, an SDP instance is specified by matrices $C, A^{(1)},A^{(2)},\ldots, A^{(m)} \in \mathbb{S}^n$, as well as $b \in \mathbb{R}^m$, and can be written as $$\begin{equation} \label{eq:SDP} \begin{align} & \min_{X \in \mathbb{S}^n} \langle C, X \rangle \\
  \text{subject to } & \langle A^{(j)}, X\rangle = b_j \text{ for } j = 1,\ldots,m \\
  & X \succeq 0 \end{align} \end{equation}$$ where $X\succeq 0$ denotes the constraint that $X$ is positive semidefinite.


In the LP or SDP case, we might also require as input parameters $R$ and $r$, where $R$ is a known upper bound on the size of the solution in the sense that $\sum_i |x_i| \leq R$ (LP) or $\text{tr}(X) \leq R$ (SDP), and where $r$ is an analogous upper bound on the size of the solution to the *dual* program (not written explicitly here, see [@apeldoorn2018ImprovedQSDPSolving]).


## Dominant resource cost/complexity

Two separate approaches to solving conic programs with quantum algorithms have been proposed in the literature. Both methods start with classical algorithms and replace some of the subroutines with quantum algorithms.


1. [Quantum interior point methods](../../quantum-algorithmic-primitives/quantum-interior-point-methods.md#quantum-interior-point-methods) (QIPMs) for LPs [@kerenidis2018QIntPoint], SOCPs [@kerenidis2019QAlgsSecondOrderConeSVM; @augustino2022inexact], and SDPs [@kerenidis2018QIntPoint; @augustino2021quantum; @huang2022fasterQuantumIPM] have been proposed. These methods start with classical interior point methods, for which the core step is solving a linear system, and simply replace the classical linear system solver with a [quantum linear system solver](../../quantum-algorithmic-primitives/quantum-linear-system-solvers.md#quantum-linear-system-solvers) (QLSS), combined with pure state [quantum tomography](../../quantum-algorithmic-primitives/quantum-tomography.md#quantum-tomography). Given a linear system $Gu = v$, the QLSS produces a quantum state $\ket{u}$, and quantum tomography is subsequently used to gain a classical estimate of the amplitudes of $\ket{u}$ in the computational basis. The QLSS ingredient introduces complexity dependence on a parameter $\kappa=\lVert G \rVert \lVert G^{-1}\rVert$, the condition number of $G$, where $\lVert \cdot \rVert$ denotes the spectral norm. Additionally, the QLSS requires that the classical data defining $G$ be loaded in the form of a [block-encoding](../../quantum-algorithmic-primitives/quantum-linear-algebra/block-encodings.md#block-encodingsClassical), for which the standard construction introduces a dependence on the factor $\zeta = \lVert G \rVert_F \lVert G \rVert^{-1}$, where $\lVert \cdot \rVert_F$ denotes the Frobenius norm. Finally, the tomography ingredient introduces a complexity dependence on a parameter $\xi$, defined as the precision to which the vector $u$ must be classically learned, measured in $\ell_2$ norm. Assuming $m$ is on the order of the number of degrees of freedom (i.e. $n$ in the case of LP and SOCP, and $n^2$ in the case of SDP), the number of queries the QIPM makes to block-encodings of the input matrices is $$\begin{align} \text{LP, SOCP \cite{augustino2022inexact}:} & \qquad \tilde{\mathcal{O}}\left(\frac{n^{1.5}\zeta \kappa}{\xi}\log(1/\epsilon)\right) \\
   \text{SDP \cite{kerenidis2018QIntPoint,augustino2021quantum}:} & \qquad \tilde{\mathcal{O}}\left(\frac{n^{2.5}\zeta \kappa}{\xi}\log(1/\epsilon)\right) \end{align}$$ where the $\tilde{\mathcal{O}}$ notation hides logarithmic factors. Note that depending on how $\xi$ is defined, extra factors of $\kappa$ may be required. Moreover, note that the complexity statements in [@augustino2021quantum] go further and analyze the worst-case dependence of $\xi$ on the overall error $\epsilon$, and additionally make the worst-case replacement $\zeta \leq \mathcal{O}\left( n \right)$. The numerical values of $\kappa$, $\zeta$, and $\xi$ are generally difficult to determine in advance for a specific application. The [block-encoding](../../quantum-algorithmic-primitives/quantum-linear-algebra/block-encodings.md#block-encodingsClassical) queries can be executed in circuit depth $\mathrm{polylog}(n+m,1/\epsilon)$, which can also be absorbed into the $\tilde{\mathcal{O}}$ notation (although it is important to note that the circuit *size* is generally $\mathcal{O}\left( n^2 \right)$). If the input matrices are sparse or given in a form other than as a list of matrix entries, there may be other more efficient [methods for block-encoding](../../quantum-algorithmic-primitives/quantum-linear-algebra/block-encodings.md#block-encodings); in this case the parameter $\zeta$ might be replaced with another parameter $\alpha > 1$, whose value would depend on the block-encoding method.
2. Quantum algorithms based on the [multiplicative weights update](../../quantum-algorithmic-primitives/multiplicative-weights-update-method.md#multiplicative-weights-update-method) (MWU) method have been proposed for SDP [@brandao2016QSDPSpeedup; @brandao2017QSDPSpeedupsLearning; @apeldoorn2017QSDPSolvers; @apeldoorn2018ImprovedQSDPSolving] and LP [@apeldoorn2017QSDPSolvers; @apeldoorn2019QAlgorithmsForZeroSumGames]. The quantum algorithm closely follows the classical algorithm based on MWU to iteratively update a candidate solution to the program. Each iteration is carried out using quantum subroutines, including [Gibbs sampling](../../quantum-algorithmic-primitives/gibbs-sampling.md#gibbs-sampling), as well as [Grover search](../../quantum-algorithmic-primitives/amplitude-amplification-and-estimation/introduction.md#amplitude-amplification-and-estimation) and quantum minimum finding [@durr1996QMinimumFinding; @apeldoorn2017QSDPSolvers] (a direct application of Grover search). Let $s$ denote the sparsity, that is, the maximum number of nonzero entries in any row or column of the matrices composing the problem input (thus, $s \leq \max(m,n)$). Then, the runtime has been upper bounded by $$\begin{align} \text{LP \cite{bouland2023zerosum}:} & \qquad \tilde{\mathcal{O}}\left(\sqrt{s}\left(\frac{rR}{\epsilon}\right)^{3.5}\right) \\
   \text{SDP \cite{apeldoorn2018ImprovedQSDPSolving}:} & \qquad \tilde{\mathcal{O}}\left(s\sqrt{m}\left(\frac{rR}{\epsilon}\right)^{4}+s\sqrt{n}\left(\frac{rR}{\epsilon}\right)^5\right) \end{align}$$ assuming sparse access to the input matrices, where $r,R$ are the parameters related to the size of the primal and dual solutions, defined above. In [@apeldoorn2018ImprovedQSDPSolving], the input model was generalized to a "quantum operator input model," based on [block-encodings](../../quantum-algorithmic-primitives/quantum-linear-algebra/block-encodings.md#block-encodings) where $s$ is replaced by the block-encoding normalization factor $\alpha$ in the runtime expressions. Note that it is possible the runtime for LP could be improved by applying the dynamic Gibbs sampling method of [@bouland2023zerosum] together with the reduction from LP to zero-sum games in [@apeldoorn2019QAlgorithmsForZeroSumGames].


The runtime expressions for the QIPM approach and the MWU approach are not directly comparable, as the former depends on instance-specific parameters $\kappa$, $\zeta$, and $\xi$, while the latter depends on instance-specific parameters $r$ and $R$. However, note that the explicit $n$-dependence is better in the case of MWU than QIPM, while the $\epsilon$-dependence is worse.


## Existing error corrected resource estimates

Neither of the approaches for conic programs have garnered study at the level of error-corrected resource estimates. Reference [@dalzell2022socp] performed a resource analysis for a QIPM at the logical level, but did not analyze additional [overheads due to error correction](../../fault-tolerant-quantum-computation/introduction.md#fault-tolerant-quantum-computation). The goal of that analysis was to completely compile the QIPM for SOCP into Clifford gates and $T$ gates, and then to numerically estimate the parameters $\kappa$, $\zeta$, and $\xi$ for the particular use case of [financial portfolio optimization](../../areas-of-application/finance/portfolio-optimization.md#portfolio-optimization), which can be reduced to SOCP. A salient feature of the QIPM is that $\mathcal{O}\left( n+m \right) \times \mathcal{O}\left( n+m \right)$ matrices of classical data must be repeatedly accessed by the QLSS via [block-encoding](../../quantum-algorithmic-primitives/quantum-linear-algebra/block-encodings.md#block-encodings), necessitating a large-scale [quantum random access memory](../../quantum-algorithmic-primitives/loading-classical-data/quantum-random-access-memory.md#quantum-random-access-memory) (QRAM) with $\mathcal{O}\left( n^2 \right)$ qubits. Accordingly, for SOCPs with $n=500$ and $m=400$ (which are still easily solved on classical computers) it was estimated that 8 million logical qubits would be needed. The total number of $T$ gates needed for the same instance size was on the order of $10^{29}$, which can be distributed over roughly $10^{24}$ layers.


We are not aware of an analogous logical resource analysis for the MWU approach to conic programming. Such an analysis would be valuable and should ideally choose a specific use case to be able to evaluate the size of all parameters involved.


## Caveats

- The QIPM approach requires a large-scale [QRAM](../../quantum-algorithmic-primitives/loading-classical-data/quantum-random-access-memory.md#quantum-random-access-memory) of size $\mathcal{O}\left( n^2 \right)$. This is a necessary ingredient to retain any hope of a speedup, and for relevant choice of $n$ the associated hardware requirements could be prohibitively large.
- The QIPM approach has a weak case for a large asymptotic speedup: even under optimal circumstances, the asymptotic speedup over classical interior point methods is less than quadratic. See the [article on the QIPM approach](../../quantum-algorithmic-primitives/quantum-interior-point-methods.md#quantum-interior-point-methods) for more information.
- The QIPM approach has a large constant prefactor that is estimated to be on the order of $10^3$ coming from state-of-the-art [QLSS](../../quantum-algorithmic-primitives/quantum-linear-system-solvers.md#quantum-linear-system-solvers) [@costa2021OptimalLinearSystem; @jennings2023QLSS]. (It is possible this could be improved using alternative approaches to QLSS such as variable-time amplitude amplification [@ambainis2010VTAA]. See also [@jennings2023QLSS].)
- The MWU approach requires a medium-scale [QRAM](../../quantum-algorithmic-primitives/loading-classical-data/quantum-random-access-memory.md#quantum-random-access-memory) of size $\mathcal{O}\left( R^2r^2/\epsilon^2 \right)$. This requirement can be avoided at the cost of an additional multiplicative overhead of $\mathcal{O}\left( R^2r^2/\epsilon^2 \right)$.
- The MWU approach has poor dependence on error $\epsilon$; for SDPs it is $\epsilon^{-5}$. Even at modest choices of $\epsilon$, this may lead the algorithm to be impractical pending significant improvements.
- A general caveat that applies to both approaches is that the appearance of instance-specific parameters makes it difficult to predict the performance of these algorithms for more specific applications.


## Comparable classical complexity and challenging instance sizes

As in the quantum case, there are multiple distinct approaches in the classical case.


1. Classical interior point methods (CIPMs): There exist fast IPM-based software implementations for solving conic programs, such as ECOS [@domahidi2013ECOS] and MOSEK [@andersen2000MOSEK]. These solvers can solve instances with thousands of variables in a matter of seconds on a standard laptop [@domahidi2013ECOS]. However, the runtime scaling is poor and scaling too far beyond this regime leads the solvers to be far less practical. The runtime of the best provably correct classical IPMs for the regime where the number of constraints is roughly equal to the number of degrees of freedom is $$\begin{align} \text{LP \cite{cohen2021LPsinMMtime}:} & \qquad \tilde{\mathcal{O}}\left(n^{\omega}\log(1/\epsilon)\right) \\
   \text{SOCP \cite{monteiro2000SOCP}:} & \qquad \tilde{\mathcal{O}}\left(n^{\omega+0.5}\log(1/\epsilon)\right) \\
   \text{SDP \cite{huang2022fasterIPM}:} & \qquad \tilde{\mathcal{O}}\left( n^{2\omega}\log(1/\epsilon)\right) \end{align}$$ where $\omega<2.37$ is the matrix multiplication exponent. It is plausible that, with some attention, the extra $n^{0.5}$ factor for SOCP could be eliminated with modern techniques. Additionally, the runtime can be somewhat reduced when the number of constraints is much less than the number of degrees of freedom; for example, the $n$-dependence of the complexity of the CIPM for SDP in [@jiang2020fasterIPM] can be as low as $\tilde{\mathcal{O}}(n^{2.5})$ when there are few constraints. On practical instances, employing techniques for fast matrix multiplication is often not beneficial, and Gaussian Elimination-like methods are used, where $\omega=3$. Note that, alternatively, by using iterative classical linear systems solvers [@strohmer2009kaczmarz], each $n^{\omega}$ factor could be replaced by a factor of $n$ at the cost of a linear dependence on $(\kappa \zeta)^2$, which could be superior if the matrices are well conditioned.
2. Classical MWU methods: a classical complexity statement for LPs is inferred from the reduction in [@apeldoorn2019QAlgorithmsForZeroSumGames] from LPs to zero-sum games and the classical analysis that appears there. For the SDP case, references in the classical literature appear only to examine specific subclasses of SDPs (e.g. [@arora2005fastAlgorithms; @arora2007SDP]). A general statement of the classical complexity for SDPs appears alongside the quantum algorithm in [@apeldoorn2017QSDPSolvers Section 2.4]. $$\begin{align} \text{LP \cite{apeldoorn2019QAlgorithmsForZeroSumGames}:} & \qquad \tilde{\mathcal{O}}\left(s\left(\frac{rR}{\epsilon}\right)^{3.5}\right) \\
   \text{SDP \cite{apeldoorn2017QSDPSolvers}:} & \qquad \tilde{\mathcal{O}}\left(s\sqrt{nm}\left(\frac{rR}{\epsilon}\right)^{4}+sn\left(\frac{rR}{\epsilon}\right)^7\right) \end{align}$$
3. Cutting-plane methods: these classical methods are used for SDPs and can outperform IPMs when the number of constraints is small. The best algorithm, based on [@lee2015FasterCuttingPlaneConvexOpt; @jiang2020CuttingPlane], has runtime $\mathcal{O}\left( m(mn^2+n^{\omega}+m^2)\log(1/\epsilon) \right)$, which can be as low as $\mathcal{O}\left( n^\omega \right)$ when $m$ is small.


It is important to note that the algorithms with the best provable complexities may not be the ones that are most useful in practice.


## Speedup

For both the IPM and the MWU approach, there can be at most a polynomial quantum speedup: upper and lower bounds scaling polynomially with $n$ are known in both the classical and quantum cases [@apeldoorn2018ImprovedQSDPSolving]. The speedup of the QIPM method depends on the scaling of $\kappa$ with $n$, but the speedup cannot be more than quadratic. The speedup of the MWU method with respect to the $n$-scaling could be as much as quadratic, assuming the sparsity is constant. There is a possibility that the speedup could be larger in practice if the [Gibbs sampling](../../quantum-algorithmic-primitives/gibbs-sampling.md#gibbs-sampling) routine is faster in practice than its worst-case upper bounds suggest, perhaps by utilizing Monte Carlo–style approaches to Gibbs sampling.


## Outlook

It is very plausible that an asymptotic polynomial speedup can be obtained in problem size using the MWU method for solving LPs or SDPs, but the speedup appears only quadratic, and an assessment of practicality depends on the scaling of certain unspecified instance-specific parameters. Similarly, the QIPM method could bring a subquadratic speedup but only under certain assumptions about the condition number of certain matrices. These quadratic and subquadratic speedups alone might be regarded as unlikely to yield practical speedups after [error correction overheads](../../fault-tolerant-quantum-computation/introduction.md#fault-tolerant-quantum-computation) and slower quantum clock speeds are considered. Future work should aim to find additional asymptotic speedups while focusing on specific practically relevant use cases that allow the unspecified parameters to be evaluated. 






[^1]: Inequality constraints of the form $Ax \leq b$ can be converted to linear equality constraints and positivity constraints by introducing a vector of slack variables $s$ and imposing $Ax + s = b$ and $s_i \geq 0$ for all $i$. An analogous trick is possible for SOCP and SDP.

