# Electronic structure problem

## Overview

We seek the energy eigenstates (or thermal states) of the Hamiltonian used to describe the electrons in molecules or material systems. The electrons interact with each other, in addition to fields produced by the nuclei (which are typically assumed to be fixed in position, and classical) and any external applied fields.


In simulations of a finite sized system, there is not a clear distinction between a "molecule" and a "material"—materials may be viewed as an extended molecule, typically with a repeating underlying atomic structure. In materials we are additionally concerned with extrapolating finite size properties to the thermodynamic limit by repeating the simulation at a range of system sizes. This enables the measurement of thermodynamic properties, such as phase diagrams. For molecular systems, we are interested in measuring microscopic properties, such as excitation energies, reaction rates, dipole moments, or nuclear forces.


One may also consider time evolution under the electronic structure Hamiltonian; this is a less well-studied problem in both classical and quantum settings, likely due to the high costs of classical simulations. As such, we will predominantly focus on static properties, commenting on dynamics simulations where relevant.


## Actual end-to-end problem(s) solved

The Hamiltonian of a system consisting of $K$ nuclei and $\eta$ electrons interacting via the Coulomb interaction is (in atomic units) $$\begin{align} H = -\sum_{i=1}^\eta \frac{(\nabla_{i})^2}{2} - \sum_{I=1}^K \frac{(\nabla_{I})^2}{2M_I} - \sum_{i,I}\frac{Z_I}{|r_{i}-R_{I}|} +\frac{1}{2}\sum_{i\neq j}\frac{1}{|r_{i}-r_{j}|} + \frac{1}{2}\sum_{I\neq J}\frac{Z_IZ_J}{|R_{I}-R_{J}|} \end{align}$$ where $\nabla$ is the derivative operator, $r_{i}$ gives the position of the $i$th electron, and $R_{I}$ and $Z_I$ give the position and charge of the $I$th nucleus. It is often appropriate to make the Born–Oppenheimer approximation, fixing the positions of the nuclei, which are treated as classical particles. The resulting electronic Hamiltonian at a fixed nuclear configuration is given by $$\begin{equation} \label{Eq:BornOppElectronic} H(\{R_{I}\}) = -\sum_i\frac{(\nabla_{i})^2}{2} - \sum_{i,I}\frac{Z_I}{|r_{i}-R_{I}|} + \frac{1}{2}\sum_{i\neq j}\frac{1}{|r_{i}-r_{j}|} + V(\{R_{I}\}) \end{equation}$$ where $V(\{R_{I}\})$ is the constant offset from the nuclear repulsion energy. This Hamiltonian can be projected onto a basis set $\{\phi_i(r)\}_{i=1}^N$ of electron spin orbital functions or grid points, and solved for the electronic eigenstates $\ket{E_i}$ or thermal state $\rho \propto e^{-\beta H}$. We note that for many molecules, the ground state of the electronic structure Hamiltonian is a good approximation for the thermal state at room temperature. This can be contrasted with the [vibrational structure of molecules](../../areas-of-application/quantum-chemistry/vibrational-structure-problem.md#vibrational-structure-problem), where excited states are also populated at room temperature. When simulating dynamics, it is necessary to use a basis set that is sufficiently flexible (or adaptive) to accurately describe the states at all times (for example, many chemical basis sets are highly optimized for ground state calculations and so are less suitable for dynamics calculations).


The electronic energy is the largest contribution to the energy of molecular/material systems in ambient conditions, and dictates the equilibrium structure and motion of the nuclei. As a result, the electronic energy eigenstates (or thermal states) often provide a good description of a wide range of system properties. Preparing the desired electronic state for a given nuclear configuration is typically the first step in learning properties of the system. We then measure the expectation values of observables with respect to these states. Properties of interest for molecular systems include:


- Energy values, potentially across a range of nuclear configurations (for electronic excitation energies at a fixed nuclear geometry, determining molecular geometries by computing the electronic ground state energy at different geometries, and finding reaction pathways & rates by computing energy differences between a sequence of geometries involved in a reaction).
- Determining transition probabilities between states (for reactions and optical properties).
- Differential changes in electronic energy in response to an applied field, for example, electronic or magnetic dipole moments, polarizability.
- Calculating forces on the nuclei, for use in molecular dynamics calculations (used in a range of applications, including protein folding and calculating drug molecule binding affinities).


Properties of interest for materials include:


- Energy densities for given system parameters (to determine phase diagrams).
- Thermodynamic properties (magnetization, thermal/electrical conductivity, bulk modulus).
- Particle densities and correlation functions between positions.


In order to understand how these observables vary as the system parameters (i.e. nuclear positions, atomic doping, temperature, applied field etc.) are changed, the desired state may need to be prepared and measured a number of times.


In dynamics simulations, one may consider how the system evolves in response to a perturbation such as that induced by an ultrafast laser pulse [@kohler1995LaserChemistry; @assion1998LaserChemicalReactions; @krausz2009AttosecondPhysics], or in particle scattering interactions.


## Dominant resource cost/complexity

#### Mapping the problem to qubits:


We discretize the electron positions by projecting onto a basis of spin orbitals. The discretization error typically decays as $1/N$ where $N$ is the number of spin orbitals used [@halkier1998BasisSet; @shepherd2012PlaneWaveConvergence] and is limited by the resolution of singularities in the Coulomb interaction at charge coalescences. A variety of functional forms have been considered for the electron orbitals (see Table [1](#Tab:ElectronicStructureMappings){reference-type="ref" reference="Tab:ElectronicStructureMappings"}). The optimal choice will be system dependent and must consider:


- The resolution of the orbital (improved by matching the character of local vs delocalized physics in the system to that of the orbital).
- The cost of computing the Hamiltonian, either in classical precomputation or (if required) coherently on a quantum device (see "Accessing the Hamiltonian," below).
- The properties of the resulting Hamiltonian (number of terms, 1-norm, locality of terms, etc.) which determine the cost of accessing the Hamiltonian in algorithms.


We can represent electronic states on a quantum computer using either first or second quantized representations.


- For $\eta$ electrons in $N$ spin orbitals, first quantization uses $\eta$ registers, which each contain $\log_2(N)$ qubits; each register enumerates which orbital its corresponding electron is in, and the wavefunction must then be antisymmetrized to respect fermionic constraints [@berry2018ImprovedEigenstatesFermionic]. The Hamiltonian of Eq. $\eqref{Eq:BornOppElectronic}$ in first quantization can be written as $$\begin{equation} H = \sum_\alpha^\eta \sum_{i,j}^N h_{ij} \ket{i}\bra{j}_\alpha + \frac{1}{2} \sum_{\alpha \neq \beta}^\eta \sum_{i,j,k,l}^N h_{ijkl} \ket{i}\bra{l}_\alpha \otimes \ket{j}\bra{k}_\beta \end{equation}$$ with one- and two-electron integrals $$\begin{align} h_{ij} & = \int dr \phi_i^*(r) \left(-\frac{(\nabla)^2}{2} - \sum_I \frac{Z_I}{|r - R_I|} \right) \phi_j(r) \\
  h_{ijkl} & = \int dr_1 dr_2 \frac{\phi_i^*(r_1) \phi_j^*(r_2) \phi_k(r_2) \phi_l(r_1)}{|r_1 - r_2|}. \end{align}$$
- In second quantization, antisymmetry is stored in the operators, which obey fermionic anticommutation relations. The Hamiltonian of Eq. $\eqref{Eq:BornOppElectronic}$ in second quantization can be written as $$\begin{equation} H = \sum_{i,j}^N h_{ij} a_i^\dag a_j + \frac{1}{2} \sum_{i,j,k,l}^N h_{ijkl} a_i^\dag a_j^\dag a_k a_l. \end{equation}$$ Under the commonly used Jordan–Wigner mapping (other mappings have also been studied, see [@mcArdle2017QuantCompuChem] for discussion) we require $N$ qubits, where each qubit stores the occupancy of the corresponding spin orbital. These mappings induce a mapping of the Hamiltonian (and other observables) to qubit operators.


<figure markdown> <span id="Tab:ElectronicStructureMappings"></span>


| **Representation** |               **Gaussians**               |                              **Plane waves**                               |                 **Bloch/Wannier functions**                  |                                           **Grids**                                           |
| :----------------: | :---------------------------------------: | :------------------------------------------------------------------------: | :----------------------------------------------------------: | :-------------------------------------------------------------------------------------------: |
|  First quantized   | [@babbush2017ExponentiallyConfigInt] [^1] | [@su2021FaultTolerantChemistryFirstQuantized; @chan2023RealSpaceChemistry] |                       Not yet studied                        | [@kivlichan2017RealSpace; @kassal2008QuantumSimChemicalDynamics; @chan2023RealSpaceChemistry] |
|  Second quantized  |    [@whitfield2011ChemistrySimulation]    |                     [@babbush2018LowDepthQSimMaterial]                     | [@ivanov2022PeriodicSolidsChemistry; @rubin2023MaterialsSim] |                         [@babbush2018LowDepthQSimMaterial Appendix A]                         |



<figcaption markdown>Table 1: Representative references (chosen based on their discussion of their choice of representation) showing the use of different basis functions in quantum algorithms for the electronic structure problem. Note, this is not intended to be a complete list of all works that have used these basis sets. </figcaption> </figure>


#### Accessing the Hamiltonian:


Quantum algorithms for the electronic structure problem require access to the Hamiltonian. This is typically provided by [block-encoding](../../quantum-algorithmic-primitives/quantum-linear-algebra/block-encodings.md#block-encodings) or [Hamiltonian simulation](../../quantum-algorithmic-primitives/hamiltonian-simulation/introduction.md#hamiltonian-simulation). For some approaches, it may be necessary to compute Hamiltonian coefficients (molecular integrals) or matrix elements coherently [@kassal2008QuantumSimChemicalDynamics; @babbush2017ExponentiallyConfigInt; @babbush2016ExponentiallySecondQuant; @babbush2019FirstQuantizedSublinear; @su2021FaultTolerantChemistryFirstQuantized; @chan2023RealSpaceChemistry], or [load them](../../quantum-algorithmic-primitives/loading-classical-data/introduction.md#loading-classical-data) from a quantum memory [@Berry2019QubitizationOfArbitraryBasisChemistry; @burg2021QuantumComputingEnhancedComputationalCataylysis; @lee2021EvenMoreEfficientChemistryTensorHyp]. As this access is often a dominant contribution to the cost of quantum algorithms, significant effort has been spent on methods of factorizing the electronic structure Hamiltonian to reduce the resources required for accessing it coherently [@motta2021lowrankrep; @Berry2019QubitizationOfArbitraryBasisChemistry; @burg2021QuantumComputingEnhancedComputationalCataylysis; @lee2021EvenMoreEfficientChemistryTensorHyp; @rubin2023MaterialsSim]. Some data-loading routines provide the ability to trade gate count for additional ancilla qubits, leading to a larger logical qubit count than required to store the system wavefunction (see the section on [loading classical data](../../quantum-algorithmic-primitives/loading-classical-data/introduction.md#loading-classical-data) for additional details).


#### State preparation:


Solving the electronic structure problem on a quantum computer reduces to the task of preparing a desired state, and measuring observables. The state to be prepared is typically an energy eigenstate, a thermal state, or a time evolved state.


- Energy eigenstates: In the following discussion, we refer to the overlap $\gamma = |\braket{\psi}{E_j}|$ between a desired eigenstate $\ket{E_j}$ and a given initial state $\ket{\psi}$, and the minimum gap $\Delta$ between the desired energy eigenvalue and other energy eigenvalues. Below, we list several methods for preparing energy eigenstates, or approximations to them.
  - Approximate eigenstates: Approximate eigenstates obtained from a classical calculation can be prepared as quantum trial states using the methods of [@tubman2018PostponingCatastrophe; @sugisaki2019MulticonfigurationalChemistryPrep], which scale as $\mathcal{O}\left( ND \right)$, where $D$ is the number of Slater determinants in the trial state. These states can be used as input for the methods below.
  - Eigenstate filtering: Methods such as those in [@lin2019OptimalQEigenstateFiltering; @lin2020NearOptimalGroundState] filter out undesired eigenstates using spectral window functions applied via [quantum singular value transformation (QSVT)](../../quantum-algorithmic-primitives/quantum-linear-algebra/quantum-singular-value-transformation.md#quantum-singular-value-transformation) to a [block-encoding](../../quantum-algorithmic-primitives/quantum-linear-algebra/block-encodings.md#block-encodings) of the Hamiltonian. The complexity to prepare the ground state (to infidelity $\epsilon$, with failure probability less than $\theta$) using this approach scales as $\widetilde{\mathcal{O}}\left( \frac{\alpha}{\gamma \Delta} \log(\theta^{-1} \epsilon^{-1}) \right)$ calls to an $(\alpha, m, 0)$-block-encoding of the Hamiltonian (where $\alpha \geq \nrm{H}$ is a normalization factor of the block-encoding). For comparison to related methods, we refer the reader to [@ge2017FasterGroundStatePrep; @lin2020NearOptimalGroundState].
  - [Adiabatic state preparation (ASP)](../../quantum-algorithmic-primitives/quantum-adiabatic-algorithm.md#quantum-adiabatic-algorithm): ASP can be used to prepare a target eigenstate (typically the ground state) by evolving from the corresponding easy-to-prepare eigenstate of an initial Hamiltonian $H(0)$ to the full electronic structure Hamiltonian $H(1)$. Time evolution can be implemented using algorithms for [Hamiltonian simulation](../../quantum-algorithmic-primitives/hamiltonian-simulation/introduction.md#hamiltonian-simulation). The total evolution time is typically chosen according to the heuristic $T \gg \max_{0 \leq s \leq 1} \nrm{\frac{dH}{ds}} / \Delta(s)^2$ where $s$ describes the adiabatic path $H(s)$ and $\Delta(s)$ is the spectral gap of $H(s)$. It is difficult to analytically bound this complexity for molecular systems (see e.g., [@reiher2017ElucidatingReactionMechanisms]) motivating numerical studies on small molecules [@veis2014AdiabaticStatePrepMethylene; @kremenetski2021AdiabaticMolecular; @lee2022isThereEvidenceChemistry; @Sugisaki2022AdiabaticMolecular].
  - [Quantum phase estimation (QPE)](../../quantum-algorithmic-primitives/quantum-phase-estimation.md#quantum-phase-estimation): The above techniques all provide methods of preparing approximate eigenstates, in some cases using promises on the gap $\Delta$, or by exploiting pre-existing knowledge of the energy eigenvalue. Given an approximate eigenstate, we can use QPE to project into the desired eigenstate and provide an estimate of the eigenenergy. QPE makes $\mathcal{O}\left( \gamma^{-2} \epsilon^{-1} \right)$ calls to a unitary $U$ encoding the spectrum of the Hamiltonian, where $\gamma = |\braket{\psi}{E_j}|$ is the overlap between the state $\ket{\psi}$ input to quantum phase estimation, and the desired energy eigenstate $\ket{E_j}$, and $\epsilon$ is the desired precision in the energy estimate. It is possible to improve the complexity to $\mathcal{O}\left( \gamma^{-1} \epsilon^{-1} \right)$ using [amplitude amplification](../../quantum-algorithmic-primitives/amplitude-amplification-and-estimation/introduction.md#amplitude-amplification-and-estimation), or to $\mathcal{O}\left( \gamma^{-2} \Delta^{-1} + \epsilon^{-1} \right)$ by exploiting knowledge of the gap $\Delta$ between the energy eigenstates to perform rejection sampling [@berry2018ImprovedEigenstatesFermionic]. The unitary encoding the Hamiltonian is typically either $U \approx e^{-iHt}$ (the approximation error must be balanced against the error from QPE) implemented via [Hamiltonian simulation](../../quantum-algorithmic-primitives/hamiltonian-simulation/introduction.md#hamiltonian-simulation), or a quantum walk operator $W$ which acts like $e^{i\arccos{H}}$ and can be implemented via [qubitization](../../quantum-algorithmic-primitives/quantum-linear-algebra/qubitization.md#qubitization) [@poulin2018SpectralQubitization; @berry2018ImprovedEigenstatesFermionic] (note that if phase estimation is performed on a qubitization operator, the output state will have the form $\frac{1}{\sqrt{2}}(\ket{E_j}\ket{0} \pm \ket{\phi_j 0^\perp})$, which reduces the success probability of obtaining the desired eigenstate by 50% [@berry2018ImprovedEigenstatesFermionic]). The costs to implement $U$ are inherited from the method used, based on the properties (commutativity, locality, number of terms, 1-norm, cost of coherently calculating coefficients) of the Hamiltonian in the chosen spin orbital basis.
- Thermal states: Several quantum algorithms have been proposed for [preparing thermal states](../../quantum-algorithmic-primitives/gibbs-sampling.md#gibbs-sampling) [@poulin2009GibbsSamplingAndEval; @chowdhury2016QGibbsSampling; @temme2011quantumMetropolis; @chen2023QThermalStatePrep]. The most efficient algorithms typically make repeated calls to a [block-encoding](../../quantum-algorithmic-primitives/quantum-linear-algebra/block-encodings.md#block-encodings) of the Hamiltonian. The complexity of these methods for concrete electronic structure problems of interest has not yet been determined. Thermal states could also be used as an approximation to the ground state, by choosing the temperature to be sufficiently low compared to the gap between the ground and first excited state [@chen2023QThermalStatePrep].
- Time evolved states: A time evolved state can be prepared using [Hamiltonian simulation algorithms](../../quantum-algorithmic-primitives/hamiltonian-simulation/introduction.md#hamiltonian-simulation), up to an error $\epsilon$. While many proposed quantum algorithms for chemistry simulation have considered using Hamiltonian simulation as a subroutine in quantum phase estimation, these have typically considered the use of Gaussian basis functions, which are not sufficiently flexible to accurately describe the time dynamics of the electrons. Classical algorithms for this task typically consider grid- or plane wave–based methods for dynamics simulations. Reference [@Babbush2023Dynamics] compared the costs of [Trotter-based](../../quantum-algorithmic-primitives/hamiltonian-simulation/product-formulae.md#product-formulae) methods [@kassal2008QuantumSimChemicalDynamics] and prior work in the interaction picture [@babbush2019FirstQuantizedSublinear; @su2021FaultTolerantChemistryFirstQuantized; @low2018HamiltonianInteractionPicture] against classical mean-field methods, finding large polynomial speedups, even for this apples-to-oranges comparison.


#### Measuring observables:


In a [fault-tolerant computation](../../fault-tolerant-quantum-computation/introduction.md#fault-tolerant-quantum-computation), it is preferable to measure observables through phase estimation-like approaches, rather than direct measurement averaging, as the former is asymptotically more efficient and can be made robust to logical errors through repetition and majority voting. Measurement schemes have been developed which achieve this using overlap estimation [@knill2007ObservableMeasurement] (which can be viewed as a special case of [amplitude estimation](../../quantum-algorithmic-primitives/amplitude-amplification-and-estimation/amplitude-estimation.md#amplitude-estimation)) or the approach of [@huggins2022ExpectationValue; @apeldoorn2022TomographyStatePreparationUnitaries] based on the [quantum gradient estimation](../../quantum-algorithmic-primitives/quantum-gradient-estimation.md#quantum-gradient-estimation) algorithm of [@gilyen2017OptQOptAlgGrad]. Both approaches require access to a state preparation unitary $U_\psi$, and its inverse[^2]. The algorithm based on overlap estimation can be formulated as performing amplitude estimation on $U_O$, a unitary [block-encoding](../../quantum-algorithmic-primitives/quantum-linear-algebra/block-encodings.md#block-encodings) of the observable $O$ with subnormalization factor $\alpha_O$. The complexity to compute the expectation value to precision $\epsilon$ is $\mathcal{O}\left( \alpha_O/\epsilon \right)$ calls to $U_O$ and $U_\psi$ (or the reflection $R_\psi = I - 2 \ket{\psi}\bra{\psi}$) and their inverses. This approach has been considered in the context of measuring: correlation functions, density of states, and linear response properties (all in [@rall2020EstimatingPhysicalQuantities]), and energy gradients with respect to various parameters (which can be used to compute forces or dipole moments, and for which a range of estimation strategies are possible) [@obrien2022MolecularForces; @steudtner2023MolecularObservables].


The gradient-based algorithm simultaneously computes the value of $M$ (noncommuting) observables $O_j$ by making $\widetilde{\mathcal{O}}\left( M^{1/2}/\epsilon \right)$ calls to $U_\psi, U_\psi^\dag$ (or $R_\psi$) and either $\widetilde{\mathcal{O}}\left( M^{3/2}/\epsilon \right)$ calls to gates of the form $e^{i x O_j}$ [@huggins2022ExpectationValue] or $\widetilde{\mathcal{O}}\left( M/\epsilon \right)$ calls to a block-encoding of the observables [@apeldoorn2022TomographyStatePreparationUnitaries]. The algorithm also requires $\mathcal{O}\left( M \log(1/\epsilon) \right)$ additional qubits. This approach has been considered in the context of measuring nuclear forces [@obrien2022MolecularForces], fermionic reduced density matrices [@huggins2022ExpectationValue] and dynamic correlation functions [@huggins2022ExpectationValue].


## Existing error corrected resource estimates

There are a large number of resource estimates for performing phase estimation to learn the ground state energies of molecular or material systems, which we list in Table [2](#Tab:ResourceEst_MolecularElectronicStructure){reference-type="ref" reference="Tab:ResourceEst_MolecularElectronicStructure"} and Table [3](#Tab:ResourceEst_MaterialElectronicStructure){reference-type="ref" reference="Tab:ResourceEst_MaterialElectronicStructure"}. These resource estimates use compilation methods described in the [fault-tolerant quantum computing](../../fault-tolerant-quantum-computation/introduction.md#fault-tolerant-quantum-computation) section. We also note the existence of a software package that provides features for calculating the non-Clifford costs of quantum phase estimation for the electronic structure problem [@Casares2022TFermionSoftwarePackage]. There are currently no results that provide resource estimates for solving a full end-to-end application (see caveats below).


<figure markdown> <span id="Tab:ResourceEst_MolecularElectronicStructure"></span>

|                      **Molecule(s)**                      |                                                                                                                        **References**                                                                                                                         |                                                      **Number of Logical qubits**                                                       |                                                              **Number of $T$/Toffoli gates**                                                               |
| :-------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------------------------------: |
|              FeMo-co<br/>(Nitrogen fixation)              | [@reiher2017ElucidatingReactionMechanisms; @Berry2019QubitizationOfArbitraryBasisChemistry; @burg2021QuantumComputingEnhancedComputationalCataylysis; @lee2021EvenMoreEfficientChemistryTensorHyp; @wan2021RandPhaseEst; @Casares2022TFermionSoftwarePackage] |                        $2196$[@lee2021EvenMoreEfficientChemistryTensorHyp] <br/>$\sim 193$[@wan2021RandPhaseEst]                        |                   $3.2 \times 10^{10}$[@lee2021EvenMoreEfficientChemistryTensorHyp] <br/> $\sim 5 \times 10^{11}$ [@wan2021RandPhaseEst]                   |
| Cytochrome P450<br/>(Biological drug metabolizing enzyme) |                                                                                                        [@goings2022ReliablyAssessingCytochromeEnzyme]                                                                                                         |                                                                 $1434$                                                                  |                                                                    $7.8 \times 10^{9}$                                                                     |
|             Lithium-ion<br/>battery molecules             |                                                                              [@kim2022FaultTolerantQuantumChemicalSimulationsLiIon; @su2021FaultTolerantChemistryFirstQuantized]                                                                              | $(10^4-10^5)$ [@kim2022FaultTolerantQuantumChemicalSimulationsLiIon] <br/>($2000 - 3000$) [@su2021FaultTolerantChemistryFirstQuantized] | $(10^{12} - 10^{14})$ [@kim2022FaultTolerantQuantumChemicalSimulationsLiIon]  <br/>    $(10^{11} - 10^{12})$ [@su2021FaultTolerantChemistryFirstQuantized] |
|                      Chromium dimer                       |                                                                                                    [@elfving2020HowQuantumComputationalAdvantageChemistry]                                                                                                    |                                                               $\sim 1300$                                                               |                                                                       $\sim 10^{10}$                                                                       |
|     Ruthenium catalyst<br/>(CO<sub>2</sub> fixation)      |                                                                                                  [@burg2021QuantumComputingEnhancedComputationalCataylysis]                                                                                                   |                                                               $\sim 4000$                                                               |                                                                  $\sim 3 \times 10^{10}$                                                                   |
|               Ibrutinib<br/>(drug molecule)               |                                                                                                              [@blunt2022ChemistryDrugDiscovery]                                                                                                               |                                                                 $2207$                                                                  |                                                                    $1.1 \times 10^{10}$                                                                    |


<figcaption markdown>Table 2: [Fault-tolerant resource estimates](../../fault-tolerant-quantum-computation/introduction.md#fault-tolerant-quantum-computation) for [quantum phase estimation](../../quantum-algorithmic-primitives/quantum-phase-estimation.md#quantum-phase-estimation) applied to a range of molecular systems. The presented gate counts are for a single run of the phase estimation circuit. QPE must be run a number of times if the overlap is $\leq 1$, and to account for rounding errors in phase estimation [@nielsen2002QCQI]. The molecules presented can have different numbers of electrons, orbitals, and classical simulation complexities, and so the results may not be directly comparable, even within a single row of the table. </figcaption> </figure>


<figure markdown> <span id="Tab:ResourceEst_MaterialElectronicStructure"></span>

|                    **Material(s)**                    |                                                                                         **References**                                                                                         |                                                                      **Number of Logical qubits**                                                                      |                                                                        **Number of $T$/Toffoli gates**                                                                         |
| :---------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|   Homogeneous electron gas<br/>(Prototypical model)   | [@babbush2018EncodingElectronicSpectraLinearT; @Kivlichan2020ImprovedFaultTolerantSimulationCondensedMatter; @mcardle2022ExploitingFermionNumber; @su2021FaultTolerantChemistryFirstQuantized] | $(1500-5000)$[@su2021FaultTolerantChemistryFirstQuantized] <br/>$\sim(100 - 1000)$ [@babbush2018EncodingElectronicSpectraLinearT; @mcardle2022ExploitingFermionNumber] | $(10^9-10^{14})$[@su2021FaultTolerantChemistryFirstQuantized] <br/> $\sim(10^8 - 10^{11})$ [@babbush2018EncodingElectronicSpectraLinearT; @mcardle2022ExploitingFermionNumber] |
|           Lithium-ion<br/>battery materials           |                                                 [@delgado2022SimulateLiIonBattery; @shokrianZini2023BatteryMaterials; @rubin2023MaterialsSim]                                                  |           ($2375 - 6652$)[@delgado2022SimulateLiIonBattery] <br/> $10^4$ [@shokrianZini2023BatteryMaterials]  <br/> $(10^5 - 10^6)$[@rubin2023MaterialsSim]            |  ($5 \times 10^{12} - 5 \times 10^{14}$) [@delgado2022SimulateLiIonBattery] <br/> $10^{15}$ [@shokrianZini2023BatteryMaterials]<br/> $(10^{12} - 10^{14})$ [@rubin2023MaterialsSim]  |
|  Condensed phase elements<br/>Lithium, Diamond, etc   |                                          [@babbush2018EncodingElectronicSpectraLinearT; @Kivlichan2020ImprovedFaultTolerantSimulationCondensedMatter]                                          |                                                  $128$ [@Kivlichan2020ImprovedFaultTolerantSimulationCondensedMatter]                                                  |                                               $(10^8 - 10^{11})$ [@Kivlichan2020ImprovedFaultTolerantSimulationCondensedMatter]                                                |
| Transition metal catalysts<br/>Nickel/Palladium Oxide |                                                                              [@ivanov2022PeriodicSolidsChemistry]                                                                              |                                                                             $10^4 - 10^5$                                                                              |                                                                              $10^{10} - 10^{13}$                                                                               |

<figcaption markdown>Table 3: [Fault-tolerant resource estimates](../../fault-tolerant-quantum-computation/introduction.md#fault-tolerant-quantum-computation) for [quantum phase estimation](../../quantum-algorithmic-primitives/quantum-phase-estimation.md#quantum-phase-estimation) applied to a range of material systems. The presented gate counts are for a single run of the phase estimation circuit. QPE must be run a number of times if the overlap is $\leq 1$, and to account for rounding errors in phase estimation [@nielsen2002QCQI]. The systems presented in a given row may be different chemical compounds, and/or can have different numbers of electrons, orbitals, and classical simulation complexities, and so the results may not be directly comparable. </figcaption> </figure>


There have been comparatively few studies of the fault-tolerant resources required for the simulation of chemical dynamics. Recent work has computed the resources required to calculate the energy loss of charged particles moving through a medium ("stopping power\"), as pertaining to nuclear fusion experiments [@rubin2023FusionDynamics]. End-to-end resource estimates were determined, including the costs of initial state preparation, measurement of observables, and repetitions across a range of parameters. The resource estimates for the end-to-end task ranged from $\sim 2000$ logical qubits and $\mathcal{O}\left( 10^{13} \right)$ Toffoli gates, to $\sim 30000$ logical qubits and $\mathcal{O}\left( 10^{17} \right)$ Toffoli gates.


## Caveats

Existing resource estimates typically consider only a single run of phase estimation and assume that we have access to the desired energy eigenstate. As outlined above, both phase estimation and eigenstate filtering scale as $\Omega{\gamma^{-1} \Delta^{-1}}$ when we have a lower bound on the gap. The "orthogonality catastrophe" suggests that the overlap of simple trial states with the desired eigenstate will decay exponentially as a function of system size. It is still an open question [@tubman2018PostponingCatastrophe; @lee2022isThereEvidenceChemistry] as to whether initial states with nonexponentially vanishing overlaps can be prepared for systems of interest. This issue may become more pressing for materials systems as we scale to the thermodynamic limit. In general, we know that the problem of finding the ground state of electronic structure Hamiltonians is QMA-hard [@whitfield2013ComplexityElectronicStructure], but it is not yet known if these complexity theoretic statements provide intuition for physically realistic Hamiltonians.


As noted above, to accurately resolve the system, a large basis set must be used (the discretization error decays as $1/N$ where $N$ is the number of spin orbitals considered). In practice, one typically repeats the calculation using increasingly accurate basis sets and then extrapolates to the continuum limit. Most quantum resource estimates to date have considered basis sets of the minimal allowable size (for exceptions, see [@kim2022FaultTolerantQuantumChemicalSimulationsLiIon; @su2021FaultTolerantChemistryFirstQuantized; @elfving2020HowQuantumComputationalAdvantageChemistry; @mcardle2022ExploitingFermionNumber; @delgado2022SimulateLiIonBattery; @shokrianZini2023BatteryMaterials; @rubin2023MaterialsSim; @rubin2023FusionDynamics]), and so underestimate the resources required to achieve sufficiently accurate results to be informative.


The end-to-end applications typically solved in the electronic structure problem can require between tens (structure determination) and millions (molecular dynamics) of energy evaluations—each with different Hamiltonian parameters that may require preparing a new state to be measured. For example, a recent analysis of quantum algorithms applied to pharmaceutical chemistry [@santagati2023DrugDesign] highlighted that to calculate the binding affinity between a drug molecule and its target (free energy differences) requires sampling a range of thermodynamic configurations, resulting in millions to billions of single-point energy evaluations. This introduces a large overhead when preparing a different state for each configuration and measuring its energy [@obrien2022MolecularForces], although alternative approaches may provide more favorable scaling [@simon2023MolecularDynamics].


## Comparable classical complexity and challenging instance sizes

The cost of exact diagonalization of the electronic structure Hamiltonian scales exponentially with the number of electrons and basis set size. As such, classical approaches to the electronic structure problem typically utilize a range of approximations that reduce their complexity to polynomial in an approximation parameter but introduce a (potentially uncontrolled) deviation from the exact ground state, leading to a bias in energy estimates and/or the expectation values of other observables. Approaches include: Hartree–Fock, density functional theory, perturbation theory, configuration interaction methods, coupled cluster methods, quantum Monte Carlo techniques, and tensor network approaches. The cheapest approaches can be applied to thousands of orbitals, but can be qualitatively inaccurate for strongly correlated systems. The most expensive approaches are more effective for strongly correlated systems, but their higher computational cost limits their applicability to roughly 100 spin orbitals. For example, [@goings2022ReliablyAssessingCytochromeEnzyme] found that a density matrix renormalization group (DMRG) calculation performed on an 86 spin orbital active space of the Cytochrome P450 enzyme molecule referenced in Table [2](#Tab:ResourceEst_MolecularElectronicStructure){reference-type="ref" reference="Tab:ResourceEst_MolecularElectronicStructure"} required around 50 hours, using 32 threads, 48 GB of RAM, and 235 GB of disk memory. We also refer to [@williams2020DirectComparisonManyBodyElectronicHamiltonians] for a comparison of 20 first-principles many-body electronic structure methods applied to a test set of seven transition metal atoms and their ions and monoxides.


Due to their extended nature, material systems are most commonly targeted with density functional theory (DFT). DFT can be applied to systems with thousands of electrons and orbitals, but can lead to uncontrolled energy bias in strongly correlated systems. Quantum Monte Carlo and tensor network methods have been successfully applied to prototypical models of material systems, and are becoming increasingly practical for more realistic models. We refer to [@leblanc2015TwoDimHubbard; @motta2017TowardsHydrogenChainManyBodyMethods; @motta2020GroundStatePropertiesHydrogenChain; @schafer2021MultiMethodHubbard] for cutting edge benchmarks of classical electronic structure methods on hydrogen chains and Hubbard models scaling to the thermodynamic limit, which act as simplified models for real materials.


## Speedup

It is nontrivial to determine the speedup of quantum algorithms for the electronic structure problem over their classical counterparts. If we consider the subtask of determining energy eigenstates, then for speedup greater than polynomial to be achieved, we require:


- The ability to prepare a trial state with nonexponentially vanishing overlap with the ground state as the system size increases.
- Polynomially scaling classical algorithms having an exponential growth in their approximation parameter (e.g., bond dimension, number of excitations) as the system size increases.


Whether these two requirements can coexist in systems of interest is an active area of research [@lee2022isThereEvidenceChemistry]. Even if exponential speedups are not available, it may be the case that quantum algorithms provide polynomial speedups over exact classical algorithms—and potentially over approximate classical algorithms.


From a complexity theoretic viewpoint, we know that simulating the dynamics of a quantum system is a BQP-complete problem [@lloyd1996UnivQSim]. Combined with the observed difficulty of classically simulating the time evolution of electronic structure Hamiltonians, this may be taken as evidence for the possibility of an exponential speedup when simulating dynamics. In [@Babbush2023Dynamics] quantum algorithms for simulating the dynamics of electrons in a grid or plane-wave basis [@kassal2008QuantumSimChemicalDynamics; @babbush2019FirstQuantizedSublinear; @su2021FaultTolerantChemistryFirstQuantized] were compared against classical methods for mean-field dynamics. Large polynomial speedups were observed, ranging from superquadratic to seventh power in the salient parameters, depending on the relation between $N$ and $\eta$.


## NISQ implementations

Solving the electronic structure problem is one of the most widely studied and touted NISQ applications. The primary NISQ approach is the [variational quantum eigensolver](../../quantum-algorithmic-primitives/variational-quantum-algorithms.md#variational-quantum-algorithms) (VQE). There have been a number of experimental demonstrations on small molecules, e.g., Refs. [@kandala2017VQE; @google2020HartreeFockVQE], as well as proposals to simulate material systems [@yoshioka2022MaterialsVQE; @manrique2020VQEmaterials2]. Related methods, such as quantum computing assisted quantum Monte Carlo methods [@huggins2022QuantumQMC] have also been developed. Nevertheless, current device noise rates are too high to enable the running of circuits sufficiently deep that they can outperform classical electronic structure methods. There is currently no evidence that heuristic NISQ approaches will be able to scale to large system sizes and provide advantage over classical methods. There have also been proposals to simulate the electronic structure problem using analog quantum simulators [@arguello2019AnalogChemistry], though to the best of our knowledge, these have not yet been experimentally demonstrated.


## Outlook

Solving the electronic structure problem has repeatedly been identified as one of the most promising applications for quantum computers. Nevertheless, the discussion above highlights a number of challenges for current quantum approaches to become practical. Most notably, after accounting for the approximations typically made (i.e. incorporating the cost of initial state preparation, using nonminimal basis sets, including repetitions for correctness checking and sampling a range of parameters), a large number of logical qubits and total $T$/Toffoli gates are required. A major difficulty is that, unlike problems such as factoring, the end-to-end electronic structure problem typically requires solving a large number of closely related problem instances.


Solving the electronic structure problem for materials is likely to be more difficult than for molecules for both classical and quantum algorithms. This is predominantly due to the larger system sizes considered. First quantized quantum algorithms may provide a promising approach to efficiently represent the large system sizes required, and their natural use of a plane wave basis is well suited to periodic material systems [@su2021FaultTolerantChemistryFirstQuantized]. Nevertheless, additional developments are required to understand how to best apply these algorithms to real systems [@shokrianZini2023BatteryMaterials]. 




[^1]: This reference is not technically a first quantized representation, as antisymmetry is stored in the operators rather than the wavefunction, but it stores states in an analogously compressed way to first quantized representations.

[^2]: Note that it can be substantially cheaper to directly execute the reflection $R_\psi = I - 2 \ket{\psi}\bra{\psi}$ used in both methods, rather than through the use of $U_\psi$, as the complexity of $R_\psi$ does not depend on the overlap $\gamma$ that appears in state preparation—see [@lin2020NearOptimalGroundState] for additional discussion.

