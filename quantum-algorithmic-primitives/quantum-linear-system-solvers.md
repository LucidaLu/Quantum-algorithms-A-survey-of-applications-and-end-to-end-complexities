# Quantum linear system solvers

## Rough overview (in words)

The goal is to solve linear systems of equations with quantum subroutines. More precisely, a *quantum linear system solver* (QLSS) takes as input an $N\times N$ complex matrix $A$ together with a complex vector $b$ of size $N$, and outputs a pure quantum state $\ket{\tilde x}$ that is an $\varepsilon$-approximation of the normalized solution vector of the linear system of equations $Ax=b$. In basic versions, QLSSs do so by loading the normalized entries of the matrix $A$ and the normalized entries of the vector $b$ into a unitary quantum circuit, either from a [quantum random access memory](../quantum-algorithmic-primitives/loading-classical-data/quantum-random-access-memory.md#quantum-random-access-memory) (QRAM) data structure, or—if the structure of $A$ and $b$ allows for this—by efficiently computing the corresponding entries on the fly.


Crucially, the number of algorithmic qubits of the linear system solver itself is only roughly $\log_2(N)$, which is exponentially smaller than the matrix size. While for general systems the number of QRAM qubits still scales with the matrix/vector size, QRAM encodings can be made more space efficient for sparse systems or can even be avoided when the corresponding entries are efficiently computable. The complexity of QLSSs depends on the condition number $\kappa(A)=\left\|A^{-1}\right\|\cdot\|A\|$ of the matrix $A$, and one then aims to give circuits with minimal quantum resource costs—such as ancilla qubits, total gate count, circuit depth, etc.—in terms of $\kappa(A)$ and the desired accuracy $\varepsilon\in(0,1)$.


## Rough overview (in math)

There are different standard input models on how the classical data from $(A,b)$ is loaded into the quantum processing unit, which are equivalent up to small polylogarithmic overhead for general matrices. We state the complexities in terms of query access of a unitary $U_b$ preparing the $n=\lceil\log_2(N)\rceil$-qubit pure quantum state $\ket{b}=\|b\|_2^{-1}\cdot\sum_{i=1}^Nb_i\ket{i}$ for $b=(b_1,\cdots,b_N)$, where $\lVert \cdot \rVert_2$ denotes the standard Euclidean vector norm, together with an $(\alpha,a,0)$-[block-encoding](../quantum-algorithmic-primitives/quantum-linear-algebra/block-encodings.md#block-encodings) $U_A$ of the matrix $A$. The QLSS problem is then stated as follows: for a triple $(U_A,U_b,\varepsilon)$ as above, the goal is to create an $n$-qubit pure quantum state $\ket{\tilde x}$ such that $$\begin{align} \label{eq:regression} \Big\|\ket{\tilde x}-\ket{x}\Big\|_2\leq\varepsilon\quad\text{for $\ket{x}=\frac{\sum_{i=1}^Nx_i\ket{i}}{\left\|\sum_{i=1}^Nx_i\ket{i}\right\|_2}$ defined by $A x=b$ with $x=(x_1,\ldots,x_N)$,} \end{align}$$ by employing as few times as possible the unitary operators $U_A,U_b$, their inverses $U_A^\dagger,U_b^\dagger$, controlled versions of $U_A,U_b$, and additional quantum gates on potentially additional ancilla qubits.


One way to think of the QLSS problem is that we seek the matrix inverse $A^{-1}$ and this can, e.g., be implemented by [quantum singular value transformation](../quantum-algorithmic-primitives/quantum-linear-algebra/quantum-singular-value-transformation.md#quantum-singular-value-transformation) (QSVT) acting on $A$ (via its [block-encoding](../quantum-algorithmic-primitives/quantum-linear-algebra/block-encodings.md#block-encodings)) with a polynomial approximation of the inverse function on the interval $[\|A\|/\kappa(A),\|A\|]$. The complexity of the corresponding scheme thereby depends on the degree of the polynomial needed for a good approximation of the inverse function on the relevant interval, and as such on the condition number $\kappa(A)$, the normalization factor $\alpha$, and the approximation error $\varepsilon$ of the resulting QLSS. In fact, it turns out that the complexity of most quantum algorithms depends on the following combined quantity $$\begin{align} \label{eq:kappa-prime} \kappa'(A):=\kappa(A)\cdot\frac{\alpha}{\|A\|}=\alpha\cdot\|A^{-1}\|, \end{align}$$ which is no smaller than $\kappa(A)$, because $\alpha\geq \|A\|$ due to the unitarity of the block-encoding. Note that in QRAM-based implementations one naturally gets $\alpha=\|A\|_F$, which then leads to linear complexity dependence on the Frobenius norm $\|A\|_F$.


As noted in [@wiebe2012QDataFitting; @chakraborty2018BlockMatrixPowers], in general we need not assume that $A$ is invertible nor that it is a square matrix, but can instead use the Moore–Penrose pseudoinverse $A^+$ of the matrix to solve the problem $\eqref{eq:regression}$ in a least-squares sense, in which case one needs to appropriately change the definition of $\kappa(A)$ to $\left\|A^{+}\right\|\cdot\|A\|$. In fact, the above QSVT-based approach directly solves this more general version of the problem [@gilyen2018QSingValTransf].


## Dominant resource cost (gates/qubits)

The state-of-the-art QLSS from [@costa2021OptimalLinearSystem] (for invertible matrices) does not directly employ the [QSVT](../quantum-algorithmic-primitives/quantum-linear-algebra/quantum-singular-value-transformation.md#quantum-singular-value-transformation) for the inverse function. Instead, it is based on discrete [adiabatic methods](../quantum-algorithmic-primitives/quantum-adiabatic-algorithm.md#quantum-adiabatic-algorithm) together with quantum eigenstate filtering based on the [QSVT](../quantum-algorithmic-primitives/quantum-linear-algebra/quantum-singular-value-transformation.md#quantum-singular-value-transformation) for a minimax polynomial [@lin2019OptimalQEigenstateFiltering]. As above, the quantum algorithm assumes access to a [block-encoding](../quantum-algorithmic-primitives/quantum-linear-algebra/block-encodings.md#block-encodings) $U_A$ of the matrix $A$ with normalization factor $\alpha$, operates on $n+5$ qubits (plus the additional qubits used for the block-encoding, discussed in more detail below), succeeds with probability roughly $1/2$, and uses $Q$ controlled queries to each of $U_A$ and $U_A^\dagger$, and $2Q$ queries to each of $U_b$ and $U_b^\dagger$, for $$\begin{align} \label{eq:kappa-scaling} Q & = \kappa'(A)\Big(C + \ln(2\varepsilon^{-1})\Big) + \mathcal{O}\left( \sqrt{\kappa'(A)} \right)=\mathcal{O}\left( \kappa'(A)\log(\varepsilon^{-1}) \right) \end{align}$$ where the constant $C$ comes from the quantitative adiabatic analysis, and there is an additional constant quantum gate overhead for each query round. The query complexity is asymptotically optimal in terms of $\kappa(A)$ [@harrow2009QLinSysSolver]. The adiabatic constant $C$ can be rigorously bounded as $C \leq \num{58617}$.[^1] Note that when $C$ is this large, the corresponding term will actually dominate the $\kappa'(A)\log(\varepsilon^{-1})$ term for practical scenarios. In recent work [@jennings2023QLSS], a version of the adiabatic approach with asymptotic complexity $\mathcal{O}\left( \kappa'(A)\log(\kappa\varepsilon^{-1}) \right)$ outperforms by close to an order of magnitude the asymptotically optimal scheme for up to $\kappa\approx10^{32}$ in terms of finite quantum resource counts.


Other known QLSSs with asymptotically worse complexities are based on [QSVT](../quantum-algorithmic-primitives/quantum-linear-algebra/quantum-singular-value-transformation.md#quantum-singular-value-transformation) [@gilyen2018QSingValTransf; @martyn2021GrandUnificationQAlgs] or [linear combination of unitaries](../quantum-algorithmic-primitives/hamiltonian-simulation/taylor-and-dyson-series-linear-combination-of-unitaries.md#taylor-and-dyson-series-linear-combination-of-unitaries) (LCU) [@childs2015QLinSysExpPrec], and are often combined with variable-time [amplitude amplification](../quantum-algorithmic-primitives/amplitude-amplification-and-estimation/amplitude-amplification.md#amplitude-amplification) (VTAA) [@ambainis2010VTAA; @ambainis2023ImprovedVTQS] for improved performance. While the known bounds on the asymptotic complexities of these methods are slightly worse with additional polylogarithmic factors, it remains open if finite size performance could be competitive (as the known upper bounds on the adiabatic constant $C$ are quite large). Moreover, to date, these VTAA-based algorithms are the only variants that are proven to solve the generic least squares (pseudoinverse) problem while achieving a close-to-optimal asymptotic scaling.


Note that if the matrix $A$ is given in a classical data structure in the computational basis, then standard ways to create the block-encoding $U_A$ make use of a [QRAM](../quantum-algorithmic-primitives/loading-classical-data/quantum-random-access-memory.md#quantum-random-access-memory) structure. For general (dense) matrices $A$, the requirement is then size $\mathcal{O}\left( N^2 \right)$ (number of qubits) with circuit depth $\mathcal{O}\left( n \right)$ for each query — or alternatively, as few as $\mathcal{O}\left( n \right)$ ancilla qubits could suffice, but at the expense of using $\mathcal{O}\left( N^2 \right)$ circuit depth [@hann2021resilienceofQRAM; @clader2022resourcesForBlockEncoding]. Initializing the depth-efficient QRAM data structure will in general also take $\mathcal{O}\left( N^2 \right)$ time. However, if $A$ is sparse, either in the computational basis [@dimatteo2020FaultTolerantQRAM], Pauli basis [@Wan2021exponentiallyfaster], or any orthonormal basis with efficiently implementable basis transformation, there are more efficient direct constructions for block-encoding $A$. Moreover, for Pauli basis access, there exist randomized QLSSs with complexity scaling as the $L_1$-norm of the Pauli coefficients [@samson2023qubitefflinalg], completely avoiding the use of block-encodings (and as such QRAM and ancilla qubits).


## Caveats

QLSSs are an important subroutine for a variety of [application areas](../areas-of-application/introduction.md#areas-of-application) of quantum algorithms. However, it is crucial to keep track of all the quantum and classical resources required and to compare these to state-of-the-art classical methods. In particular, the following factors should be taken into account:


- The classical precomputation complexities for the eigenstate filtering routine are neglected, but can be kept efficient in practice [@dong2020efficientPhaseFindingInQSP].
- The size of the adiabatic constant $C$ is expected to be about an order of magnitude better than stated above, but at least in the asymptotically optimal approach not more than one order of magnitude [@costa2021OptimalLinearSystem].
- When needed, the [QRAM](../quantum-algorithmic-primitives/loading-classical-data/quantum-random-access-memory.md#quantum-random-access-memory) cost can be prohibitive, if it requires the full overhead of [quantum error correction and fault tolerance](../fault-tolerant-quantum-computation/introduction.md#fault-tolerant-quantum-computation) [@hann2021resilienceofQRAM], especially for QRAMs of maximum size $\mathcal{O}\left( N^2 \right)$ qubits, required for general (dense) matrices.
- In the formulation of the QLSS problem, the pure quantum state $\ket{x}$ corresponds to the normalized solution vector of the linear system $Ax=b$. While the normalization factor can be obtained as well, this comes at the price of added complexity scaling as $\widetilde{\mathcal{O}}\left( n\kappa'(A)\varepsilon^{-1} \right)$ [@chakraborty2018BlockMatrixPowers Corollary 32].
- QLSSs do not produce a classical description of the solution vector $x$ or an approximation thereof, but rather the pure quantum state $\ket{\tilde x}$. In order to obtain a classical approximation of the vector $x$, one needs to combine QLSSs with pure state [quantum tomography](../quantum-algorithmic-primitives/quantum-tomography.md#quantum-tomography), which can be performed using $\mathcal{O}\left( N\varepsilon^{-2} \right)$ samples. If $\text{poly}(n)$ query-cost QRAM is also available, then the complexity can be quadratically improved in terms of the precision using optimized pure state tomography [@apeldoorn2022TomographyStatePreparationUnitaries], or alternatively the overall complexity may be further improved using *iterative refinement* to $\mathcal{O}\left( Ns (s+\frac{\kappa^2(A)}{\|A\|})\text{polylog}(N/\varepsilon) \right)$ as described in [@mohammadisiahroudi2022QEnhancedRegression], where $s$ is the maximum number of nonzero elements of $A$ in any row or column.
- The overall complexities $\widetilde{\mathcal{O}}\left( N\kappa'(A)\varepsilon^{-1} \right)$ and $\mathcal{O}\left( Ns (s+\frac{\kappa^2(A)}{\|A\|})\text{polylog}(N/\varepsilon) \right)$ (where we generously allow $\text{poly}(n)$ query-cost QRAM) to obtain a classical description of the solution can be compared to classical textbook Gaussian elimination–based computation, which leads to complexity $\mathcal{O}\left( N^3 \right)$ or more precisely $\mathcal{O}\left( N^\omega \right)$ with $\omega\in[2,2.372)$ denoting the matrix multiplication exponent. Further, QLSSs should also be compared with state-of-the-art randomized solvers. For example, the randomized Kaczmarz method with standard classical access to the matrix elements returns an $\varepsilon$-approximation of the vector $x$, while scaling as $\mathcal{O}\left( s\kappa_F^2(A)\log(\varepsilon^{-1}) \right)$ for $s$ row-sparse matrices and $\kappa_F(A)=\left\|A^{-1}\right\|\cdot\|A\|_F$. Moreover, if $A$ is $s$-sparse and positive semidefinite (PSD), then using the conjugate gradient method one can obtain a solution in time $\mathcal{O}\left( Ns\sqrt{\kappa(A)}\log(\varepsilon^{-1}) \right)$ [@hackbusch1994IterativeSolLargeSparseLE Chapter 10.2], which can be generalized to the least-squares problem (and thus non-Hermitian matrices) at the cost of a quadratically worse condition number dependence $\mathcal{O}\left( Ns\kappa\log(\kappa(A)/\varepsilon) \right)$ by considering the modified equation $A^\dagger A x = A^\dagger b$. As such, it seems that the QLSS may not provide a superquadratic speedup when a full classical solution is to be extracted, and even subquadratic speedups seem to be limited to a narrow parameter regime.
- Quantum-inspired methods [@chia2019SampdSubLinLowRankFramework; @gilyen2020ImprovedQInspiredAlgorithmForRegression] that start from a classical data structure intended to mimic QRAM—allowing to sample from probability distributions with probabilities proportional to the squared magnitudes of elements in a given row of $A$—give samples from an $\varepsilon$-approximation to the solution vector in (dimension free) complexity $\mathcal{O}\left( \kappa_F^4(A)\kappa^2(A)\varepsilon^{-2} \right)$ [@Shao2022FQILSS; @gilyen2020ImprovedQInspiredAlgorithmForRegression], and can be used to compute an approximate solution by repeated sampling. Note that while the required data structure is classical, it might still be prohibitively expensive to build when the matrix $A$ is huge.
- When it comes to classical methods, solvers that depend on the condition number are useful in practice whenever combined with preconditioners [@Saad2002IterativeSLS]. However, the performance of preconditioners is often only heuristic, and using preconditioners for QLSS is not (yet) explored in-depth [@clader2013preconditioned; @Shao2018Qpre-conditionerLSS; @tong2020PrecQLinSysSolvAndMatFunEval].


## Example use cases

- [Quantum interior point methods](../quantum-algorithmic-primitives/quantum-interior-point-methods.md#quantum-interior-point-methods) in convex optimization and its corresponding applications [@kerenidis2018QIntPoint; @mohammadisiahroudi2023efficientQLSAinIPMandLP]
- [Quantum machine learning](../areas-of-application/machine-learning-with-classical-data/introduction.md#machine-learning-with-classical-data) applications [@wiebe2012QDataFitting; @rebentrost2014QSVM]
- [Solving differential equations](../areas-of-application/solving-differential-equations.md#solving-differential-equations) and corresponding applications, e.g., for the finite element method that does not require a tomography step [@montanaro2016quantum]


## Further reading

- Original QLSS (termed HHL) [@harrow2009QLinSysSolver]
- For a recent overview discussion of QLSS, see [@an2022QLSStimeDepAdiabatic]
- State-of-the-art QLSS based on discrete adiabatic methods [@costa2021OptimalLinearSystem] 






[^1]: This number is derived from applying [@costa2021OptimalLinearSystem Theorem 9] with $\sqrt{2-\sqrt{2}}\times \num{44864} \times \kappa$ steps, each of which incurs one call to the block-encoding, such that the output is guaranteed to have overlap at least $1/\sqrt{2}$ with the ideal state. Eigenstate filtering then succeeds with probability at least $1/2$; accounting for the need to repeat twice on average, one arrives at a constant $\num{117235}$, matching [@jennings2023QLSS Eq. (L2)].

